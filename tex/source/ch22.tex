As discussed in the previous chapter, the goal of reinforcement learning is to determine closed-loop control policies that result in the maximization of an accumulated reward, and RL algorithms are generally classified as either model-based or model-free. In both cases it is generally assumed that the reward function is known, and both typically rely on collecting system data to either update a learned model (model-based), or directly update a learned value function or policy (model-free).

While successful in many settings, these approaches to RL also suffer from several drawbacks. First, determining an appropriate reward function that can accurately represent the true performance objectives can be challenging\footnote{RL agents can sometimes learn how to exploit a reward function without actually producing the desired behavior. This is commonly referred to as \textit{reward hacking}. Consider training an RL agent with a reward for each piece of trash collected. Rather than searching the area to find more trash (the desired behavior), the agent may decide to throw the trash back onto the ground and pick it up again!}. Second, rewards may be \textit{sparse}, which makes the learning process expensive in terms of both the required amount of data and in the number of failures that may be experienced when exploring with a suboptimal policy\footnote[][\baselineskip]{This issue of sparse rewards is less relevant if data is cheap, for example when training in simulation.}. This chapter introduces the \textit{imitation learning} approach to RL, where a reward function is not assumed to be known \textit{a priori} but rather it is assumed the reward function is described implicitly through expert demonstrations. 


\notessection{Imitation Learning}
The formulation of the imitation learning problem is quite similar to the RL problem formulation from the previous chapter. The main difference is that instead of leveraging an explicit reward function $r_t = R(\x_t, \bu_t)$ it will be assumed that a set of demonstrations from an expert are provided.

\subsection{Problem Formulation}
It will be assumed that the system is a Markov Decision Process (MDP) with a state $\x$ and control input $\bu$, and the set of admissible states and controls are denoted as $\mathcal{X}$ and $\mathcal{U}$\marginnote{The field of RL often uses $\bm{s}$ to express the state and $\bm{a}$ to represent an action, but $\x$ and $\bu$ will be used here for consistency with previous chapters.}. The system dynamics are expressed by the probabilistic transition model:
\begin{equation} \label{eq:RLmodel2}
p(\x_t \mid \x_{t-1}, \bu_{t-1}),
\end{equation}
which is the conditional probability distribution over $\x_t$, given the previous state and control. As in the previous chapter, the goal is to define a \textit{policy} $\pi$ that defines the closed-loop control law\footnote{This chapter will consider a stationary policy for simplicity.}:
\begin{equation}
\bu_t = \pi(\x_t).
\end{equation}

The primary difference in formulation from the previous RL problem is that we do not have access to the reward function, and instead we have access to a set of expert demonstrations where each demonstration $\xi$ consists of a sequence of state-control pairs:
\begin{equation} \label{eq:expert}
\xi = \{(\x_0, \bu_0), (\x_1, \bu_1), \dots\},
\end{equation}
which are drawn from the expert policy $\pi^*$.
The imitation learning problem is therefore to determine a policy $\pi$ that imitates the expert policy $\pi^*$:
\begin{definition}[Imitation Learning Problem]
For a system with transition model \eqref{eq:RLmodel2} with states $\x \in \mathcal{X}$ and controls $\bu \in \mathcal{U}$, the imitation learning problem is to leverage a set of demonstrations $\Xi = \{\xi_1, \dots, \xi_D \}$ from an expert policy $\pi^*$ to find a policy $\hat{\pi}^*$ that imitates the expert policy.
\end{definition}

There are generally two approaches to imitation learning: the first is to directly learn how to imitate the expert's policy and the second is to indirectly imitate the policy by instead learning the expert's reward function.
This chapter will first introduce two classical approaches to imitation learning (\textit{behavior cloning} and the \textit{DAgger} algorithm) that focus on directly imitating the policy. Then a set of approaches for learning the expert's reward function will be discussed, which is commonly referred to as \textit{inverse reinforcement learning}. The chapter will then conclude with a couple of short discussions into related topics on learning from experts (e.g. through comparisons or physical feedback) as well as on interaction-aware control.

\subsection{Behavior Cloning}
Behavior cloning approaches use a set of expert demonstrations $\xi \in \Xi$ to determine a policy $\pi$ that imitates the expert. This can be accomplished through supervised learning techniques, where the difference between the learned policy and expert demonstrations are minimized with respect to some metric. Concretely, the goal is to solve the optimization problem:
\begin{equation*}
\hat{\pi}^* = \underset{\pi}{\arg\min} \:\:  \sum_{\xi \in \Xi}\sum_{\x \in \xi} L(\pi(\x), \pi^*(\x)),
\end{equation*}
where $L$ is the cost function\footnote{Different loss functions could include $p$-norms (e.g. Euclidean norm) or $f$-divergences (e.g. KL divergence) depending on the form of the policy.}, $\pi^*(\x)$ is the expert's action for at the state $\x$, and $\hat{\pi}^*$ is the approximated policy.

However this approach may not yield very good performance since the learning process is only based on a set of samples provided by the expert. In many cases these expert demonstrations will not be uniformly sampled across the entire state space and therefore it is likely that the learned policy will perform poorly when not close to states found in $\xi$. This is particularly true when the expert demonstrations come from a \textit{trajectory} of sequential states and actions, such that the \textit{distribution} of the sampled states $\x$ in the dataset is defined by the expert policy. Then, when an estimated policy $\hat{\pi}^*$ is used in practice it produces its own distribution of states that will be visited, which will likely not be the same as in the expert demonstrations! This distributional mismatch leads to compounding errors, which is a major challenge in imitation learning.

\subsection{DAgger: Dataset Aggregation}
One straightforward idea for addressing the issue of distributional mismatch in states seen under the expert policy and the learned policy is to simply collect new expert data as needed\footnote{Assuming the expert can be queried on demand.}. In other words, when the learned policy $\hat{\pi}^*$ leads to states that aren't in the expert dataset just query the expert for more information! The behavioral cloning algorithm that leverages this idea is known as DAgger\cite{RossGordonEtAl2011} (Dataset Aggregation).

\begin{algorithm}[ht]
 \KwData{$\pi^*$}
 \KwResult{$\hat{\pi}^*$}
 $\mathcal{D} \xleftarrow{} 0$\\
 Initialize $\hat{\pi}$\\
 \For{$i=1$ \KwTo $N$}{
  $\pi_i = \beta_i \pi^* + (1-\beta_i)\hat{\pi}$\\
  Rollout policy $\pi_i$ to sample trajectory $\tau = \{\x_0, \x_1, \dots \}$\\
  Query expert to generate dataset $\mathcal{D}_i = \{(\x_0, \pi^*(\x_0)), (\x_1, \pi^*(\x_1)), \dots\}$\\
  Aggregate datsets, $\mathcal{D} \xleftarrow{} \mathcal{D} \cup \mathcal{D}_i$\\
  Retrain policy $\hat{\pi}$ using aggregated dataset $\mathcal{D}$
 }
 \Return $\hat{\pi}$
 \caption{DAgger: Dataset Aggregation}
 \label{alg:dagger}
\end{algorithm}

As can be seen in Algorithm \ref{alg:dagger}, this approach iteratively improves the learned policy by collecting additional data from the expert. This is accomplished by rolling out the current learned policy for some number of time steps and then asking the expert what actions they would have taken at each step along that trajectory. Over time this process drives the learned policy to better approximate the true policy and reduce the incidence of distributional mismatch. One disadvantage to the approach is that at each step the policy needs to be retrained, which may be computationally inefficient.


\subsection{Inverse Reinforcement Learning} \label{subsec:irl}
Approaches that learn policies to imitate expert actions can be limited by several factors:
\begin{enumerate}
    \item Behavior cloning provides no way to understand the underlying reasons for the expert behavior (no reasoning about outcomes or intentions).
    \item The ``expert'' may actually be suboptimal\footnote{Although the discussion of inverse RL in this section will also assume the expert is optimal, there exist approaches to remove this assumption.}.
    \item A policy that is optimal for the expert may not be optimal for the agent if they have different dynamics, morphologies, or capabilities.
\end{enumerate}
An alternative approach to behavioral cloning is to reason about and try to learn a representation of the underlying reward function $R$ that the expert was using to generate its actions. By learning the expert's intent, the agent can potentially outperform the expert or adjust for differences in capabilities\footnote{Learned reward representations can potentially generalize across different robot platforms that tackle similar problems!}. This approach (learning reward functions) is known as \textit{inverse reinforcement learning}.

Inverse RL approaches assume a specific parameterization of the reward function, and in this section the fundamental concepts will be presented by parameterizing the reward as a linear combination of (nonlinear) features:
\begin{equation*}
R(\x, \bu) = \w^\top  \phi(\x, \bu),
\end{equation*}
where $\w \in \R^n$ is a weight vector and $\phi(\x, \bu): \mathcal{X} \times \mathcal{U} \xrightarrow{} \R^n$ is a feature map. For a given feature map $\phi$, the goal of inverse RL can be simplified to determining the weights $\w$.
Recall from the previous chapter on RL that the total (discounted) reward under a policy $\pi$ is defined for a time horizon $T$ as:
\begin{equation*}
V_T^\pi(\x) = E\big[\sum_{t=0}^{T-1} \gamma^tR(\x_t, \pi(\x_t)) \mid \x_0 = \x \big].
\end{equation*}
Using the reward function $R(\x, \bu) = \w^\top  \phi(\x, \bu)$ this value function can be expressed as:
\begin{equation*}
V_T^\pi(\x) = \w^\top  \mu(\pi, \x), \quad \mu(\pi, \x) = E_{\pi}\big[\sum_{t=0}^{T-1} \gamma^t \phi(\x_t, \pi(\x_t)) \mid \x_0 = \x \big],
\end{equation*}
where $\mu(\pi, \x)$ is defined by an expectation over the trajectories of the system under policy $\pi$ (starting from state $\x$) and is referred to as the \textit{feature expectation}\footnote{Feature expectations are often computed using a Monte Carlo technique (e.g. using the set of demonstrations for the expert policy).}. One insight that can now be leveraged is that by definition the optimal expert policy $\pi^*$ will always produce a greater value function:
\begin{equation*}
    V_T^{\pi^*}(\x) \geq V_T^\pi(\x), \quad \forall \x \in \mathcal{X}, \quad \forall \pi,
\end{equation*}
which can be expressed in terms of the feature expectation as:
\begin{equation} \label{eq:irlcondition}
 {\w^*}^\top  \mu(\pi^*, \x) \geq  {\w^*}^\top  \mu(\pi, \x), \quad \forall \x \in \mathcal{X}, \quad \forall \pi.
\end{equation}
Theoretically, identifying the vector $\w^*$ associated with the expert policy can be accomplished by finding a vector $\w$ that satisfies this condition. 
However this can potentially lead to ambiguities! For example, the choice $\w = 0$ satisfies this condition trivially!
In fact, reward ambiguity is one of the main challenges associated with inverse reinforcement learning\cite{NgRussell2000}. The algorithms discussed in the following chapters will propose techniques for alleviating this issue.

\subsubsection{Apprenticeship Learning}
The apprenticeship learning\cite{AbbeelNg2004} algorithm attempts to avoid some of the problems with reward ambiguity by leveraging an additional insight from condition \eqref{eq:irlcondition}. Specifically, the insight is that it doesn't matter how well $\w^*$ is estimated as long as a policy $\pi$ can be found that \textit{matches the feature expectations}. Mathematically, this conclusion is derived by noting that:
\begin{equation*}
\begin{split}
\lVert \mu(\pi, \x) - \mu(\pi^*, \x) \rVert_2 \leq \epsilon \implies \lvert \w^\top  \mu(\pi, \x) - \w^\top  \mu(\pi^*, \x) \rvert \leq \epsilon
\end{split}
\end{equation*}
for any $\w$ as long as $\lVert \w \rVert_2 \leq 1$. In other words, as long as the feature expectations can be matched then the performance will be as good as the expert \textit{even if the vector $\w$ does not match $\w^*$}. Another practical aspect to the approach is that it will be assumed that the initial state $\x_0$ is drawn from a distribution $D$ such that the value function is also considered in expectation as:
\begin{equation*}
E_{\x_0 \sim D}\big[ V_T^\pi(\x_0) \big] = \w^\top  \mu(\pi), \quad \mu(\pi) = E_{\pi}\big[\sum_{t=0}^{T-1} \gamma^t \phi(\x_t, \pi(\x_t)) \big].    
\end{equation*}
This is useful to avoid having to consider all $\x \in \mathcal{X}$ when matching features\footnote{Trying to find a policy that matches features for every possible starting state $\x$ is likely intractable or even infeasible.}.

To summarize, the goal of the apprenticeship learning approach is to find a policy $\pi$ that matches the feature expectations with respect to the expert policy (i.e. makes $\mu(\pi)$ as similar as possible to $\mu(\pi^*)$)\footnote{See Example \ref{ex:apprentice} for an example of why matching features is intuitively useful.}. This is accomplished through Algorithm \ref{alg:apprentice}, which uses an iterative approach to finding better policies.
\begin{algorithm}[ht]
 \KwData{$\mu(\pi^*)$, $\epsilon$}
 \KwResult{$\hat{\pi}^*$}
 Initialize policy $\pi_0$\\
 \For{$i=1$ \KwTo $\dots$}{
  Compute $\mu(\pi_{i-1})$ (or approximate via Monte Carlo)\\
  Solve problem \eqref{eq:irlcomputew} with policies $\{\pi_0, \dots, \pi_{i-1}\}$ to compute $\w_i$ and $t_i$
  \begin{equation} \label{eq:irlcomputew}
    \begin{split}
    (\w_i, t_i) = \underset{\w, t}{\arg\max} \:\: & t,\\
    \text{s.t.} \:\:& \w^\top \mu(\pi^*) \geq \w^\top \mu(\pi) + t, \quad \forall \pi \in  \{\pi_0, \dots, \pi_{i-1}\},\\
    & \lVert \w \rVert_2 \leq 1.
    \end{split}
  \end{equation}
  \If{$t_i \leq \epsilon$}{
    $\hat{\pi}^* \xleftarrow{}$ best feature matching policy from $\{ \pi_0, \dots, \pi_{i-1} \}$\\
    \Return $\hat{\pi}^*$
  }
  Use RL to find an optimal policy $\pi_i$ for reward function defined by $\w_i$\\
 }
 \caption{Apprenticeship Learning}
 \label{alg:apprentice}
\end{algorithm}

To better understand this algorithm it is useful to further examine the optimization problem \eqref{eq:irlcomputew}\footnote{This problem can be thought of as an inverse RL problem that is seeking to find the reward function vector $\w$ such that the expert \textit{maximally outperforms} the other policies.}. Suppose that instead of making $\w$ a decision variable it was actually fixed, then the resulting optimization would be:
\begin{equation*}
\begin{split}
t^*(\w) = \underset{t}{\max} \:\: & t,\\
\text{s.t.} \:\:& \w^\top \mu(\pi^*) \geq \w^\top \mu(\pi) + t, \quad \forall \pi \in  \{\pi_0, \pi_1, \dots\},\\
\end{split}
\end{equation*}
which is essentially computing the smallest performance loss among the candidate policies $\{\pi_0, \pi_1, \dots\}$ with respect to the expert policy, \textit{assuming the reward function weights are $\w$}. If $\w$ was known, then if $t^*(\w) \leq \epsilon$ it would guarantee that one of the candidate policies would effectively perform as well as the expert. 

Since $\w$ is not known, the actual optimization problem \eqref{eq:irlcomputew} maximizes the smallest performance loss across \textit{all vectors $\w$ with $\lVert \w \rVert_2 \leq 1$}. Therefore, if $t_i \leq \epsilon$ (i.e. the termination condition in Algorithm \ref{alg:apprentice}), then there must be a candidate policy whose performance loss is small \textit{for all possible choices of $\w$}! In other words, there is a candidate policy that matches feature expectations well enough that good performance can be guaranteed without assuming the reward function is known, and without attempting to estimate the reward accurately.



\begin{example}[Apprenticeship Learning vs. Behavioral Cloning] \label{ex:apprentice}
Consider a problem where the goal is to drive a car across a city in as short of time as possible. In the imitation learning formulation it is assumed that the reward function is not known, but that there is an expert who shows how to drive across the city (i.e. what routes to take). A behavioral cloning approach would simply try to mimic the actions taken by the expert, such as memorizing that whenever the agent is at a particular intersection it should turn right. Of course this approach is not robust when at intersections that the expert never visited!

The apprenticeship learning approach tries to avoid the inefficiency of behavioral cloning by instead identifying features of the expert's trajectories that are more generalizable, and developing a policy that experiences the same feature expectations as the expert. For example it could be more efficient to notice that the expert takes routes without stop signs, or routes with higher speed limits, and then try to find policies that also seek out those features!
\end{example}

\subsubsection{Maximum Margin Planning}
The maximum margin planning approach\cite{RatliffBagnellEtAl2006} uses an optimization-based approach to computing the reward function weights $\w$ that is very similar to \eqref{eq:irlcomputew} but with some additional flexibility. In its most standard form the MMP optimization is:
\begin{equation*} \label{eq:mmp_simple}
\begin{split}
\hat{w}^* = \underset{\w}{\arg\min} \:\: & \lVert \w \rVert_2^2,\\
\text{s.t.} \:\:& w^\top \mu(\pi^*) \geq w^\top \mu(\pi) + 1, \quad \forall \pi \in  \{\pi_0, \pi_1, \dots\}.
\end{split}
\end{equation*}
Again this problem computes the reward function vector $\w$ such that the expert policy \textit{maximally outperforms} the policies in the set $\{\pi_0, \pi_1, \dots\}$.

However the formulation is also improved in two ways: it adds a slack term to account for potential expert suboptimality and it adds a similarity function that gives more ``margin'' to policies that are dissimilar to the expert policy. This new formulation is:
\begin{equation} \label{eq:mmp}
\begin{split}
\hat{w}^* = \underset{\w, v}{\arg\min} \:\: & \lVert \w \rVert_2^2 + Cv,\\
\text{s.t.} \:\:& w^\top \mu(\pi^*) \geq w^\top \mu(\pi) + m(\pi^*, \pi) - v, \quad \forall \pi \in  \{\pi_0, \pi_1, \dots\},
\end{split}
\end{equation}
where $v$ is a slack variable that can account for expert suboptimality, $C > 0$ is a hyperparameter that is used to penalize the amount of assumed suboptimality, and $m(\pi^*, \pi)$ is a function that quantifies how dissimilar two policies are.

One example of where this formulation is advantageous over the apprenticeship learning formulation \eqref{eq:irlcomputew} is when the expert is suboptimal. In this case it is possible that there is no $\w$ that makes the expert policy outperform all other policies, such that the optimization \eqref{eq:irlcomputew} returns $\w_i = 0$ and $t_i = 0$ (which is obviously not the appropriate solution). Alternatively the slack variables in the MMP formulation allow for a reasonable $\w$ to be computed. 

\subsubsection{Maximum Entropy Inverse Reinforcement Learning}
While the apprenticeship learning approach shows that matching feature counts is a necessary and sufficient condition to ensure a policy performs as well as an expert, it also has some ambiguity (similar to the reward weight ambiguity problem discussed before). This ambiguity is associated with the fact that there could be different policies that lead to the same feature expectations! 

This issue can also be thought of in a slightly more intuitive way in terms of distributions over trajectories. Specifically, a policy $\pi$ induces a distribution over trajectories\footnote{This distribution can be visualized as a set of paths generated by simulating the system many times with policy $\pi$ (i.e. using a Monte Carlo method).} $\tau = \{(\x_0, \pi(\x_0)), (\x_1, \pi(\x_1)), \dots \}$ that is denoted as $p_{\pi}(\tau)$. The feature expectations can be rewritten in terms of this distribution as:
\begin{equation*}
\mu(\pi) = E_{\pi}\big[f(\tau) \big] = \int p_{\pi}(\tau)f(\tau) d\tau,
\end{equation*}
where $f(\tau) = \sum_{t=0}^{T-1} \gamma^t \phi(\x_t, \pi(\x_t))$.
Now suppose a policy $\pi$ was found that matched feature expectations\footnote{For example by using apprenticeship learning.} with an expert policy $\pi^*$ such that:
\begin{equation*}
\begin{split}
\int p_{\pi}(\tau)f(\tau) d\tau = \int p_{\pi^*}(\tau)f(\tau) d\tau.
\end{split}
\end{equation*}
Crucially this condition is not sufficient to guarantee that $p_{\pi}(\tau) = p_{\pi^*}(\tau)$ (which would be ideal). In fact, the distribution $p_{\pi}(\tau)$ could also have an \textit{arbitrary preference} for some paths that is \textit{unrelated} to the feature matching objective.

The main idea in the maximum entropy inverse RL approach\cite{ZiebartMaasEtAl2008} is to not only match the feature expectations, but also remove ambiguity in the path distribution $p_{\pi}(\tau)$ by trying to make $p_{\pi}(\tau)$ \textit{as broadly uncommitted as possible}. In other words, find a policy that matches feature expectations but otherwise has no additional path preferences. This concept is known as the maximum entropy principle\footnote{A maximum entropy distribution can be thought of as the \textit{least informative distribution} of a class of distribution. This is useful in situations where it is undesirable to encode unintended prior information.}.

The maximum entropy IRL approach finds a minimally preferential, feature expectation matching distribution by solving the optimization problem:
\begin{equation} \label{eq:maxentIRL}
\begin{split}
p^*(\tau) = \underset{p}{\arg\max} \:\: & \int -p(\tau) \log p(\tau) d\tau,\\
\text{s.t.} \:\:& \int p(\tau)f(\tau) d\tau = \int p_{\pi^*}(\tau)f(\tau) d\tau, \\
& \int p(\tau) d\tau = 1, \\
& p(\tau) \geq 0, \quad \forall \tau,
\end{split}
\end{equation}
where the objective is the mathematical definition of a distribution's entropy, the first constraint requires feature expectation matching, and the remaining constraints ensure that $p(\tau)$ is a valid probability distribution. It turns out that the solution to this problem has the exponential form:
\begin{equation*}
p^*(\tau, \blam) = \frac{1}{Z(\blam)}e^{\blam^\top  f(\tau)}, \quad Z(\blam) = \int e^{\blam^\top  f(\tau)} d\tau,
\end{equation*}
where $Z(\blam)$ normalizes the distribution, and where $\blam$ must be chosen such that the feature expectations match:
\begin{equation*}
\int p^*(\tau, \blam)f(\tau) = \int  p_{\pi^*}(\tau)f(\tau) d\tau.
\end{equation*}
In other words the maximum entropy IRL approach tries to find a distribution parameterized by $\blam$ that match features, but also requires that the distribution $p^*(\tau, \blam)$ belong to the exponential family.

To determine the value of $\blam$ that matches features, it is assumed that the expert also selects trajectories with high reward with exponentially higher probability:
\begin{equation*}
p_{\pi^*}(\tau) \propto e^{{\w^*}^\top  f(\tau)},
\end{equation*}
and therefore ideally $\blam = \w^*$.
Of course $\w^*$ (and more generally $p_{\pi^*}(\tau)$) are not known, and therefore a maximum likelihood estimation approach is used to compute $\blam$ to best approximate $\w^*$ based on the sampled expert demonstrations\footnote{By assuming the expert policy is also exponential, the maximum likelihood estimate is theoretically \textit{consistent} (i.e. $\blam \xrightarrow{} \w^*$ as the number of demonstrations approaches infinity).}.

In particular, an estimate $\hat{\w}^*$ of the reward weights is computed from the expert demonstrations $\Xi = \{\xi_0, \xi_1, \dots \}$ (which each demonstration $\xi_i$ is a trajectory) by solving the maximum likelihood problem:
\begin{equation*}
\begin{split}
\hat{\w}^* &= \underset{\blam}{\arg\max} \prod_{\xi_i \in \Xi} p^*(\xi_i, \blam), \\
&= \underset{\blam}{\arg\max} \sum_{\xi_i \in \Xi} \blam^\top  f(\xi_i) - \log Z(\blam), \\
\end{split}
\end{equation*}
which can be solved using a gradient descent algorithm where the gradient is computed by:
\begin{equation*}
\nabla_{\blam} J(\blam) = \sum_{\xi_i \in \Xi} f(\xi_i) - E_{\tau \sim p^*(\tau, \blam)}[f(\tau)].  
\end{equation*}
The first term of this gradient is easily computable since the expert demonstrations are known, and the second term can be approximated through Monte Carlo sampling. However, this Monte Carlo sampling estimate is based on sampling trajectories from the distribution $p^*(\tau, \blam)$. This leads to a iterative algorithm:
\begin{enumerate}
    \item Initialize $\blam$ and collect the set of expert demonstrations $\Xi = \{\xi_0, \xi_1, \dots\}$.
    \item Compute the optimal policy\footnote{For example through traditional RL methods.} $\pi_{\blam}$ with respect to the reward function with $\w = \blam$.
    \item Using the policy $\pi_{\blam}$, sample trajectories of the system and compute an approximation of $E_{\tau \sim p^*(\tau, \blam)}[f(\tau)]$.
    \item Perform a gradient step on $\blam$ to improve the maximum likelihood cost.
    \item Repeat until convergence.
\end{enumerate}

To summarize, the maximum entropy inverse reinforcement learning approach identifies a distribution over trajectories that matches feature expectations with the expert, but by restricting the distribution to belong to the exponential family ensures that spurious preferences (path preferences not motivated by feature matching) are not introduced. Additionally, this distribution over trajectories is parameterized by a value that is an estimate of the reward function weights.

\subsection{Learning From Comparisons and Physical Feedback}
Both behavioral cloning and inverse reinforcement learning approaches rely on expert demonstrations of behavior. However in some practical scenarios it may actually be difficult for the expert to provide complete/quality demonstrations. For example it has been shown\cite{BasuYangEtAl2017} that when humans are asked to demonstrate good driving behavior in simulation they retroactively think their behavior was too aggressive! As another example, if a robot has a high-dimensional control or state space it could be difficult for the expert to specify the full high-dimensional behavior. Therefore another interesting question in imitation learning is to find a way to learn from alternative data sources besides complete demonstrations.

\subsubsection{Learning from Comparisons}
One alternative approach is to use \textit{pairwise comparisons}\cite{SadighDraganEtAl2017}, where an expert is shown two different behaviors and then asked to rank which behavior is better. Through repeated queries it is possible to converge to an understanding of the underlying reward function. For example, suppose two trajectories $\tau_A$ and $\tau_B$ are shown to an expert and that trajectory $\tau_A$ is preferred. Then assuming that the reward function is:
\begin{equation*}
R(\tau) = \w^\top  f(\tau),
\end{equation*}
where $f(\tau)$ are the collective feature counts (same as in Section \ref{subsec:irl}), this comparison can be used to conclude that:
\begin{equation*}
\w^\top  f(\tau_A) > \w^\top  f(\tau_B).
\end{equation*}
In other words, this comparison has split the space of possible reward weights $\w$ in half through the hyperplane:
\begin{equation*}
(f(\tau_A) - f(\tau_B))^\top  \w = 0.
\end{equation*}
By continuously querying the expert with new comparisons\footnote{The types of comparisons shown can be selectively chosen to maximally split the remaining space of potential $\w$ in order to minimize the total number of expert queries that are required.}, the space of possible reward weights $\w$ will continue to shrink until a good estimate of $\w^*$ can be made. In practice the expert decision may be a little noisy and therefore the hyperplanes don't define hard cutoffs, but rather can be used to ``weight'' the possible reward vectors $\w$.

\subsubsection{Learning from Physical Feedback}
Another alternative to learning from complete expert demonstrations is to simply allow the expert to physically interact with the robot to correct for undesirable behavior\cite{BajcsyLoseyEtAl2017}. In this approach, a physical interaction (i.e. a correction) is assumed to occur when the robot takes actions that result in a lower reward than the expert's action. 

For a reward function of the form $R(\x, \bu) = \w^\top  \phi(\x, \bu)$ the robot maintains an estimate of the reward weights $\hat{\w}^*$ and the expert is assumed to have act according to a true set of optimal weights $\w^*$. Suppose the robot's policy, which is based on the estimated reward function with weights $\hat{\w}^*$, yields a trajectory $\tau_{R}$. Then, if the expert physically interacts with the robot to make a correction the resulting actual trajectory $\tau_{H}$ is assumed to satisfy:
\begin{equation*}
{\w^*}^\top f(\tau_H) \geq {\w^*}^\top f(\tau_R),
\end{equation*}
which simply states that the reward of the new trajectory is higher. This insight is then leveraged in a maximum a posteriori approach for updating the estimate $\hat{\w}^*$ after each interaction. Specifically, this update takes the form:
\begin{equation*}
\hat{\w}^* \xleftarrow{} \hat{\w}^* + \beta(\f(\tau_H) - \f(\tau_R)),
\end{equation*}
where $\beta > 0$ is a scalar step size. The robot then uses the new estimate to change its policy, and the process iterates. Note that this idea yields an approach that is similar to the concept of matching feature expectations from inverse reinforcement learning, except that the approach is iterative rather than requiring a batch of complete expert demonstrations.


\subsection{Interaction-aware Control and Intent Inference}
Yet another interesting problem in robot autonomy arises when robots and humans are interacting to accomplish shared or individual goals. Many classical examples of this problem arise in autonomous driving settings, when human-driven vehicles interact with autonomous vehicles in settings such as highway merging or at intersections. While the imitation learning problems from the previous sections are focused on understanding the expert's behavior for the purpose of \textit{imitating} the behavior, in this setting the human's behavior needs to be understood in order to ensure safe interactions. However there is an additional component to understanding interactions: \textit{the robot's behavior can influence the human's behavior}\footnote{It is particularly important in interaction-aware robot control to understand the effects of the robot's actions on the human's behavior. Otherwise the human's could simply be modeled as dynamic obstacles!}.

\subsubsection{Interaction-aware Control with Known Human Model}
One common approach is to model the interaction between humans and robots as a dynamical system that has a combined state $\x$, where the robot controls are denoted $\bu_R$ and the human decisions or inputs are denoted as $\bu_H$. The transition model is therefore defined as:
\begin{equation*}
p(\x_t \mid \x_{t-1}, \bu_{R, t-1}, \bu_{H, t-1}).
\end{equation*}
In other words the interaction dynamics evolve according to the actions taken by both the robot and the human. In this interaction the robot's reward function is denoted as $R_R(\x, \bu_R, \bu_H)$ and the human's reward function is denoted as $R_H(\x, \bu_R, \bu_H)$, which are both functions of the combined state and both agent's actions\footnote{While $R_R$ and $R_H$ do not have to be the same, choosing $R_R = R_H$ may be desirable for the robot to achieve human-like behavior.}. 

Under the assumption that both the robot and the human act optimally\footnote{While not necessarily true, this assumption is important to make the resulting problem formulation tractable to solve in practice.} with respect to their cost functions:
\begin{equation*}
\begin{split}
\bu_R^*(\x) &= \underset{\bu_R}{\arg\max} \:\: R_R(\x, \bu_R, \bu_H^*(\x)), \\
\bu_H^*(\x) &= \underset{\bu_H}{\arg\max} \:\: R_H(\x, \bu_R^*(\x), \bu_H). \\
\end{split}
\end{equation*}
Additionally, assuming both reward functions $R_R$ and $R_H$ are known\footnote{The reward function $R_H$ could be approximated using inverse reinforcement learning techniques.}, computing $\bu_R^*$ is still extremely challenging due to the two-player game dynamics of the decision making process. However this problem can be made more tractable by modeling it as a \textit{Stackelberg game}, which restricts the two-player game dynamics to a leader-follower structure. Under this assumption it is assumed that the robot is the ``leader'' and that as the follower the human acts according to:
\begin{equation} \label{eq:hri_humanpolicy}
\bu_H^*(\x, \bu_R) = \underset{\bu_H}{\arg\max} \:\: R_H(\x, \bu_R, \bu_H).
\end{equation}
In other words the human is assumed to see the action taken by the robot \textit{before} deciding on their own action.
The robot policy can therefore be computed by solving:
\begin{equation} \label{eq:hri_robotpolicy}
\bu_R^*(\x) = \underset{\bu_R}{\arg\max} \:\: R_R(\x, \bu_R, \bu_H^*(\x, \bu_R)),
\end{equation}
which can be solved using a gradient descent approach. For the gradient descent approach the gradient of:
\begin{equation*}
J(\x, \bu_R) = R_R(\x, \bu_R, \bu_H^*(\x, \bu_R)),
\end{equation*}
can be computed using the chain rule as:
\begin{equation*}
\frac{\partial J}{\partial \bu_R} = \frac{\partial R_R}{\partial \bu_R} + \frac{\partial R_R}{\partial \bu_H^*}\frac{\partial \bu_H^*}{\partial \bu_R}.
\end{equation*}
Since the reward function $R_R$ is known the terms $\partial R_R/\partial \bu_R$ and $\partial R_R/\partial \bu_H^*$ can be easily determined. In order to compute the term $\partial \bu_H^*/\partial \bu_R$, which represents how much the robot's actions impact the human's actions, an additional step is required. First, assuming the human acts optimally according to \eqref{eq:hri_humanpolicy} the necessary optimality condition is:
\begin{equation*}
g(\x, \bu_R, \bu_H^*) = 0, \quad g = \frac{\partial R_H}{\partial \bu_H},
\end{equation*}
which for the fixed values of $\x$ and $\bu_R$ specifies $\bu_H^*$.
Then, by implicitly differentiating this condition with respect to the robot action $\bu_R$:
\begin{equation*}
\frac{\partial g}{\partial \bu_R} + \frac{\partial g}{\partial \bu_H^*}\frac{\partial \bu_H^*}{\partial \bu_R} = 0,
\end{equation*}
which can be used to solve for:
\begin{equation*}
\frac{\partial \bu_H^*}{\partial \bu_R}(\x, \bu_R, \bu_H^*) = -\Big(\frac{\partial g}{\partial \bu_H^*}\Big)^{-1}\frac{\partial g}{\partial \bu_R}.
\end{equation*}
Notice that every term in this expression can be computed\footnote{Assuming the human's reward function is known.} and therefore it can be substituted into the gradient calculation:
\begin{equation*}
\frac{\partial J}{\partial \bu_R} = \frac{\partial R_R}{\partial \bu_R} - \frac{\partial R_R}{\partial \bu_H^*}\Big(\frac{\partial g}{\partial \bu_H^*}\Big)^{-1}\frac{\partial g}{\partial \bu_R},
\end{equation*}
which can then be computed as long as it is possible to compute $\bu_H^*(\x, \bu_R)$.

To summarize, one approach to interaction-aware control is to model the interaction as a Stackelberg game, where it is assumed that both the human and the robot act optimally with respect to some reward functions. This formulation of the problem enables the robot to choose actions based on an implicit understanding of how the human will react.

\subsubsection{Intent Inference}
One disadvantage to the approach for interaction-aware control from the previous section is that it assumes the human acts optimally with respect to a \textit{known} reward function. While a reward function could be learned through inverse reinforcement learning, this is not practical for real-world settings where different humans behave differently. Returning to the example of interaction between human drivers and autonomous vehicles, the human could exhibit drastically different behavior depending on whether they have an aggressive or passive driving style. In these settings the problem of \textit{intent inference} focuses on identifying underlying behavioral characteristics that can lead to more accurate behavioral models\footnote{This problem can be formulated as a partially observable Markov decision process (POMDP) since the underlying behavioral characteristic is not directly observable, yet influences the system's behavior.}.

One approach to intent inference\cite{SadighLandolfiEtAl2018} is to model the underlying behavioral differences through a set of unknown parameters $\btheta$ which need to be inferred by observing the human's behavior. Mathematically this is expressed by defining the human's reward function $R_H(\x, \bu_R, \bu_H, \btheta)$ to be a function of $\btheta$, and assuming the human chooses actions according to:
\begin{equation*}
p(\bu_H \mid \x, \bu_R, \btheta) \propto e^{R_H(\x, \bu_R, \bu_H, \btheta)}.
\end{equation*}
In other words this model assumes the human is exponentially more likely to pick optimal actions\footnote{This assumption was also used in the Maximum Entropy IRL approach.}, but that they may pick suboptimal actions as well.

The objective of intent inference is therefore to estimate the parameters $\btheta$, which can be accomplished through Bayesian inference methods. In the Bayesian approach a \textit{probability distribution} over parameters $\btheta$ is updated based on observations. Specifically the belief distribution is denoted as $b(\btheta)$, and given an observation of the human's actions $\bu_H$ the belief distribution is updated as:
\begin{equation*}
b_{t+1}(\btheta) = \frac{1}{\eta}p(\bu_{H,t} \mid \x_t, \bu_{R,t}, \btheta)b_t(\btheta),
\end{equation*}
where $\eta$ is a normalizing constant. This Bayesian update is simply taking the prior belief over $\btheta$ and updating the distribution based on the likelihood of observing human action $\bu_H$ under that prior. Note that this concept is quite similar to the concepts of inverse reinforcement learning: a set of parameters that describe the human's (experts) behavior are continually updated when new observations of their actions are gathered.

While the robot could sit around and \textit{passively} observe the human act to collect samples for the Bayesian updates, it is often more efficient for the robot to \textit{probe} the human to take interesting actions that are more useful for revealing the intent parameters $\btheta$. This can be accomplished by choosing the robot's reward function to be:
\begin{equation*}
R_R(\x, \bu_R, \bu_H, \btheta) = I(b(\btheta), \bu_R) + \lambda R_{\text{goal}}(\x, \bu_R, \bu_H, \btheta)
\end{equation*}
where $\lambda>0$ is a tuning parameter and $I(b(\btheta), \bu_R)$ denotes a function that quantifies the amount of information gained with respect to the belief distribution from taking action $\bu_R$. In other words the robot's reward is a tradeoff between exploiting the current knowledge of $\btheta$ to accomplish the objective and taking exploratory actions to improve the intent inference. With this robot reward function the robot's actions are chosen to maximize the expected reward:
\begin{equation*}
\bu_R^*(\x) = \underset{\bu_R}{\arg\max} \:\: E_{\btheta}[R_R(\x, \bu_R, \bu_H, \btheta)].
\end{equation*}

To summarize, this robot policy will try to simultaneously accomplish the robot's objective and gather more information to improve the inference of the human's intent (modeled through the parameters $\btheta$). In a highway lane changing scenario this type of policy might lead the robot to nudge into the other lane to see if the other car will slow down (passive driving behavior) or try to block the lane change (aggressive driving behavior). Once the robot has a strong enough belief about the human's behavior it may choose to either complete the lane change or slow down to merge behind the human driver.
